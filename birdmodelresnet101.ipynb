{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1435929,"sourceType":"datasetVersion","datasetId":841289}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport random\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger()\n\n# Paths\nroot = '/kaggle/input/bird-species-classification-220-categories'\ntrain_path = root + '/Train'\ntest_path = root + '/Test'\n\n# Get labels\ntrain_labels = [i for i in os.listdir(train_path)]\ntest_labels = [i for i in os.listdir(test_path)]\n\ntmp_train_img_paths = [train_path + '/' + j for j in train_labels]\ntmp_test_img_paths = [test_path + '/' + j for j in test_labels]\ntrain_img_paths, test_img_paths = [], []\n\nfor i in tmp_train_img_paths:\n    for j in os.listdir(i):\n        train_img_paths.append(i + '/' + j)\nfor i in tmp_test_img_paths:\n    for j in os.listdir(i):\n        test_img_paths.append(i + '/' + j)\ndel tmp_test_img_paths, tmp_train_img_paths\n\nrandom.shuffle(train_img_paths)\nrandom.shuffle(test_img_paths)\n\n# label2name, name2label\nlabel2name, name2label = {}, {}\nfor i in range(len(train_labels)):\n    label2name[i] = train_labels[i]\n    name2label[train_labels[i]] = i\n\n# ground truth\ntrain_labels, test_labels = [], []\nfor i in train_img_paths:\n    train_labels.append(name2label[i.split('/')[-2]])\nfor j in test_img_paths:\n    test_labels.append(name2label[j.split('/')[-2]])\n\n# Transforms\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\n# Dataset\nclass BirdDataset(Dataset):\n    def __init__(self, data, labels, transform=None):\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item_index):\n        img_path = self.data[item_index]\n        img_ground_truth = self.labels[item_index]\n        img_data = cv2.imread(img_path)\n        #img\n        img_data = cv2.imread(img_path)\n        if self.transform:\n            H, W, C = img_data.shape\n            # print(H, W, C)\n            T, B, L, R = 0, 0, 0, 0 # top bottom left right\n            if H > W: # cao hơn rộng => padding chiều rộng\n                R = H-W\n            elif H < W: #rộng hơn cao => padding chiều cao\n                T = W-H\n            else: #bang nhau\n                pass\n            # print(R, T)\n            transform = transforms.Compose(\n                [\n                    transforms.ToPILImage(),\n                    transforms.Pad(padding=(L, T, R, B), fill=0, padding_mode='constant'), #trái - trên - phải - dưới\n                    transforms.RandomRotation(degrees=5),\n                    transforms.RandomHorizontalFlip(p=0.5),\n                    transforms.ColorJitter(\n                        brightness=0.05,\n                        contrast=0.05,\n                        saturation=0.05,\n                        hue=0.02\n                    ),\n\n                    transforms.Resize((224, 224)),\n                    transforms.ToTensor(),\n                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                ]\n            )\n            img_data_tensor = transform(img_data)        \n        img_ground_truth_tensor = torch.tensor(img_ground_truth, dtype=torch.long)\n        return img_data_tensor, img_ground_truth_tensor\n\n\n# Dataset + Dataloader\ntrain_dataset = BirdDataset(train_img_paths, train_labels, transform=transform)\ntest_dataset = BirdDataset(test_img_paths, test_labels, transform=transform)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n\n# Model\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = models.resnet101(pretrained=True)\n\nfor param in model.parameters():\n    param.requires_grad = True\n\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, len(label2name))\nmodel = model.to(device)\n\n# Loss & optimizer\nloss_func = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\nepochs = 20\nbest_accuracy = 0.0\n\n# Training loop\nfor epoch in range(epochs):\n    print(f'Epoch {epoch + 1}/{epochs}')\n    model.train()\n    running_loss = 0.0\n    correct, total = 0, 0\n\n    for batch_data in train_dataloader:\n        img_data, label_data = batch_data\n        img_data, label_data = img_data.to(device), label_data.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(img_data)\n        loss = loss_func(outputs, label_data)\n\n        predictions = outputs.argmax(dim=1)\n        accuracy = sum(predictions == label_data) / len(label_data)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        correct += (predictions == label_data).sum().item()\n        total += label_data.size(0)\n\n    epoch_loss = running_loss / len(train_dataloader)\n    epoch_accuracy = correct / total\n    print(f\"Train Loss: {epoch_loss:.6f}, Accuracy: {epoch_accuracy:.6f}\")\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    val_correct, val_total = 0, 0\n\n    with torch.no_grad():\n        for batch_data in test_dataloader:\n            img_data, label_data = batch_data\n            img_data, label_data = img_data.to(device), label_data.to(device)\n            outputs = model(img_data)\n            loss = loss_func(outputs, label_data)\n\n            predictions = outputs.argmax(dim=1)\n            val_correct += (predictions == label_data).sum().item()\n            val_total += label_data.size(0)\n            val_loss += loss.item()\n\n    test_loss = val_loss / len(test_dataloader)\n    test_accuracy = val_correct / val_total\n    print(f\"Test Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}\")\n\n    if test_accuracy > best_accuracy:\n        best_accuracy = test_accuracy\n        torch.save(model.state_dict(), f'best_model{epoch}.pth')\n        print(f\"Saved best model! Accuracy: {best_accuracy:.6f}\")\n\nprint(f'Best accuracy achieved = {best_accuracy}')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:25:13.387882Z","iopub.execute_input":"2025-04-30T16:25:13.388474Z","iopub.status.idle":"2025-04-30T16:33:24.672263Z","shell.execute_reply.started":"2025-04-30T16:25:13.388455Z","shell.execute_reply":"2025-04-30T16:33:24.671403Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\nTrain Loss: 4.317054, Accuracy: 0.142447\nTest Loss: 3.302098, Test Accuracy: 0.278854\nSaved best model! Accuracy: 0.278854\nEpoch 2/20\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1370437616.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":15}]}